# Default values for hami-vgpu.

nameOverride: ""
fullnameOverride: "hami"
namespaceOverride: ""
imagePullSecrets: [ ]
version: "v2.5.0"

#Nvidia GPU Parameters
resourceName: "nvidia.com/gpu"
resourceMem: "nvidia.com/gpumem"
resourceMemPercentage: "nvidia.com/gpumem-percentage"
resourceCores: "nvidia.com/gpucores"
resourcePriority: "nvidia.com/priority"

#MLU Parameters
mluResourceName: "cambricon.com/vmlu"
mluResourceMem: "cambricon.com/mlu370.smlu.vmemory"
mluResourceCores: "cambricon.com/mlu370.smlu.vcore"

#Hygon DCU Parameters
dcuResourceName: "hygon.com/dcunum"
dcuResourceMem: "hygon.com/dcumem"
dcuResourceCores: "hygon.com/dcucores"

#Iluvatar GPU Parameters
iluvatarResourceName: "iluvatar.ai/vgpu"
iluvatarResourceMem: "iluvatar.ai/MR-V100.vMem"
iluvatarResourceCore: "iluvatar.ai/MR-V100.vCore"

#Metax SGPU Parameters
metaxResourceName: "metax-tech.com/sgpu"
metaxResourceCore: "metax-tech.com/vcore"
metaxResourceMem: "metax-tech.com/vmemory"

#Enflame GCU Parameters
enflameResourceName: "enflame.com/gcu"

#kunlunxin XPU Parameters
xpuResourceName: "kunlunxin.com/xpu"
xpuResourceMem: "kunlunxin.com/xpu-memory"

#alibaba PPU Parameters
alibabaResourceName: "alibabacloud.com/ppu"

overwriteEnv: false


schedulerName: "hami-scheduler"

podSecurityPolicy:
  enabled: false

license: cO2syd/KBSfMYzLzJaVZISPFhCj/P5aym3hoXVnp6POkfzyqFp7JI7XZNRH9I/tyIIaQU+Msh4zSBnDof8E6QFJVizdgtDFqsW0JClBXzVr5iPBR2QTkXaRYqtt+ImLTHczWPF2tHex5ghf8QYRoRqLpTq3JaDgEA4dJt3LantcOz6+pOx6qaJ5AMI5jrOIkhzV5kL9mZOKTd3kgNipDiEYGte0ArQjqoBSscnHhVKO5AgsV60BWDmu2tKg3UwQPnwcLxMIHY6H4XsOvCQQ1FScQfM87e2lYDn01X2mtMC780Bh7zq8ahSQkP9jKEgbwB/CXF5O0YmMCkaVW3dKnva0XooVHVAVHc6YgMrzbPSxPzlvrN0XmLcl1k16CUwRV9QjrkRuz+sqaIxglDV+a7CBV0kVeyxbrtOLCoC/WVP+FC8FjHru5iYFcZMivCls7HEazbD2/f+LGtJPQPR4tgoAiRFsQLkAu/jhP78Y/cBxx7lRzzA18R81S76COy5WC

nonOverwrite:
  - resourcePoolCM
  - flavorCM
  - devicePluginCM
  - licenseCM

global:
  gpuHookPath: /usr/local
  labels: {}
  annotations: {}
  managedNodeSelectorEnable: false
  managedNodeSelector:
    usage: "gpu"
  prometheus:
    serviceMonitorSelector:
      release: prometheus



scheduler:
  # @param nodeName defines the node name and the nvidia-vgpu-scheduler-scheduler will schedule to the node.
  # if we install the nvidia-vgpu-scheduler-scheduler as default scheduler, we need to remove the k8s default
  # scheduler pod from the cluster first, we must specify node name to skip the schedule workflow.
  nodeName: ""
  #nodeLabelSelector:
  #  "gpu": "on"
  overwriteEnv: "false"
  defaultSchedulerPolicy:
    nodeSchedulerPolicy: binpack
    gpuSchedulerPolicy: spread
    nodeSchedulerUsageEnable: false
    gpuSchedulerUsageEnable: false
  metricsBindAddress: ":9395"
  livenessProbe: false
  leaderElect: true
  # when leaderElect is true, replicas is available, otherwise replicas is 1.
  replicas: 1

  # HAMI Enterprise Characteristics
  resourcePoolSchedulingEnabled: false
  resourcePoolConfigMapName: resource-pools-cm
  flavorConfigMapName: flavors-cm
  virtualDeviceConfigMapName: virtual-devices-cm
  prometheusAddress: "http://prometheus-kube-prometheus-prometheus.prometheus:9090"
  prometheusTimeout: "10s"
  # @param enableSchedulerLogCache enable scheduler log cache
  enableSchedulerLogCache: true
  # @param schedulerLogServerBindAddress defines the network address and port for the scheduler log API server to bind to.
  # Format: ":<port>" or "host:<port>". Use ":<port>" (e.g., ":9396") to bind to all network interfaces on the specified port.
  # Default: ":9396"
  schedulerLogServerBindAddress: ":9396"
  # @param schedulerLogMaxCachePod defines the maximum number of Pod scheduling logs to cache.
  # The scheduler stores logs for recently scheduled Pods in memory to serve the
  # /api/v1/pod/{namespace}/{name}/schedulerlog API. This parameter controls the
  # upper limit of this in-memory cache. When the number of cached Pods exceeds
  # this value, the oldest entries are automatically evicted (LRU policy).
  # Default: 1000
  schedulerLogMaxCachePod: 1000

  kubeScheduler:
    # @param enabled indicate whether to run kube-scheduler container in the scheduler pod, it's true by default.
    enabled: true
    image: registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler
    imageTag: "v1.23.17"
    imagePullPolicy: IfNotPresent
    resources: {}
      # If you do want to specify resources, uncomment the following lines, adjust them as necessary.
      # and remove the curly braces after 'resources:'.
#      limits:
#        cpu: 1000m
#        memory: 1000Mi
#      requests:
#        cpu: 100m
#        memory: 100Mi
    extraNewArgs:
      - --config=/config/config.yaml
      - -v=4
    extraArgs:
      - --policy-config-file=/config/config.json
      - -v=4
  extender:
    image: "harbor.rise.io/poc/hami"
    imagePullPolicy: IfNotPresent
    resources: {}
      # If you do want to specify resources, uncomment the following lines, adjust them as necessary,
      # and remove the curly braces after 'resources:'.
#      limits:
#        cpu: 1000m
#        memory: 1000Mi
#      requests:
#        cpu: 100m
#        memory: 100Mi
    extraArgs:
      - --debug
      - -v=4
  podAnnotations: {}
  #nodeSelector:
  #  gpu: "on"
  tolerations: []
  #serviceAccountName: "hami-vgpu-scheduler-sa"
  admissionWebhook:
    customURL:
      enabled: false
      # must be an endpoint using https.
      # should generate host certs here
      host: 127.0.0.1 # hostname or ip, can be your node'IP if you want to use https://<nodeIP>:<schedulerPort>/<path>
      port: 31998
      path: /webhook
    whitelistNamespaces:
    # Specify the namespaces that the webhook will not be applied to.
      # - default
      # - kube-system
      # - istio-system
    reinvocationPolicy: Never
    failurePolicy: Ignore
  patch:
    image: docker.io/jettech/kube-webhook-certgen:v1.5.2
    imageNew: liangjw/kube-webhook-certgen:v1.1.1
    imagePullPolicy: IfNotPresent
    priorityClassName: ""
    podAnnotations: {}
    nodeSelector: {}
    tolerations: []
    runAsUser: 2000
  service:
    type: NodePort  # Default type is NodePort, can be changed to ClusterIP
    httpPort: 443   # HTTP port
    schedulerPort: 31998  # NodePort for HTTP
    monitorPort: 31993    # Monitoring port
    schedulerLogPort: 31994
    schedulerLogTargetPort: 9396
    labels: {}
    annotations: {} 

devicePlugin:
  image: "harbor.rise.io/poc/hami"
  monitorimage: "harbor.rise.io/poc/hami"
  monitorctrPath: /usr/local/vgpu/containers
  imagePullPolicy: IfNotPresent
  deviceSplitCount: 10
  deviceMemoryScaling: 1
  deviceCoreScaling: 1
  runtimeClassName: ""
  migStrategy: "none"
  disablecorelimit: "false"
  passDeviceSpecsEnabled: false
  extraArgs:
    - -v=4
  
  service:
    type: NodePort  # Default type is NodePort, can be changed to ClusterIP
    httpPort: 31992
    labels: {}
    annotations: {}

  pluginPath: /var/lib/kubelet/device-plugins
  libPath: /usr/local/vgpu

  podAnnotations: {}
  nvidianodeSelector:
    gpu: "on"
  tolerations: []
  # The updateStrategy for DevicePlugin DaemonSet.
  # If you want to update the DaemonSet by manual, set type as "OnDelete".
  # We recommend use OnDelete update strategy because DevicePlugin pod restart will cause business pod restart, this behavior is destructive.
  # Otherwise, you can use RollingUpdate update strategy to rolling update DevicePlugin pod.
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1

  resources: {}
    # If you do want to specify resources, uncomment the following lines, adjust them as necessary.
    # and remove the curly braces after 'resources:'.
#    limits:
#       cpu: 1000m
#       memory: 1000Mi
#    requests:
#      cpu: 100m
#      memory: 100Mi

  vgpuMonitor:
    extraArgs:
      - --device-config-file=/device-config.yaml
      - -v=4
    resources: {}
      # If you do want to specify resources, uncomment the following lines, adjust them as necessary.
      # and remove the curly braces after 'resources:'.
#      limits:
#        cpu: 1000m
#        memory: 1000Mi
#      requests:
#        cpu: 100m
#        memory: 100Mi

  # HAMI Enterprise Characteristics
  serviceMonitor:
    enabled: true
    interval: 15s
    honorLabels: false
    additionalLabels:
      release: prometheus
    relabelings: [ ]

devices:
  mthreads:
    enabled: false
    customresources:
      - mthreads.com/vgpu
  nvidia:
    gpuCorePolicy: default
  ascend:
    enabled: false
    image: "quanzhenglong.com/camp/ascend-device-plugin:v2.4.3"
    imagePullPolicy: IfNotPresent
    extraArgs: []
    nodeSelector:
      ascend: "on"
    tolerations:
      - key: "arm64"
        operator: "Equal"
        value: "true"
        effect: "NoSchedule"
    customresources:
      - huawei.com/Ascend910A
      - huawei.com/Ascend910A-memory
      - huawei.com/Ascend910B2
      - huawei.com/Ascend910B2-memory
      - huawei.com/Ascend910B3
      - huawei.com/Ascend910B3-memory
      - huawei.com/Ascend910B4
      - huawei.com/Ascend910B4-memory
      - huawei.com/Ascend310P
      - huawei.com/Ascend310P-memory
    ascend310P: "pro"
  kunlunxin:
    enabled: false
    image: "harbor.rise.io/poc/xpu-device-plugin:03111257"
    imagePullPolicy: IfNotPresent
    nodeSelector:
      xpu: "on"
    tolerations: {}
    # XPU-P800 OAM, XPU-P800 PCIE
    cardType: "XPU-P800 OAM"
    topologyPairs:
      - 1,2,3,4
      - 0,2,3,5
      - 0,1,3,6
      - 0,1,2,7
      - 0,5,6,7
      - 1,4,6,7
      - 2,4,5,7
      - 3,4,5,6
  iluvatar:
    enabled: false
    image: "quanzhenglong.com/iluvatar/gpu-managerii:v4.3.0-2.1-x86_64"
    imagePullPolicy: IfNotPresent
    nodeSelector:
      iluvatar: "on"
    tolerations: {}
    logLevel: 4
    service:
      servicePort: 32022
      containerPort: 5679
    customresources:
      - iluvatar.ai/BI-V150
      - iluvatar.ai/BI-V150.vMem
      - iluvatar.ai/BI-V150.vCore 
      - iluvatar.ai/BI-V150S
      - iluvatar.ai/BI-V150S.vMem
      - iluvatar.ai/BI-V150S.vCore
      - iluvatar.ai/BI-V100
      - iluvatar.ai/BI-V100.vMem
      - iluvatar.ai/BI-V100.vCore
      - iluvatar.ai/MR-V100
      - iluvatar.ai/MR-V100.vMem
      - iluvatar.ai/MR-V100.vCore
      - iluvatar.ai/MR-V50
      - iluvatar.ai/MR-V50.vMem
      - iluvatar.ai/MR-V50.vCore
  cambricon:
    enabled: false
    image: "harbor.4pd.io/rise-io/camp/cambricon-device-plugin:v2.0.17"
    imagePullPolicy: IfNotPresent
    nodeSelector:
      mlu: "on"
    tolerations: {}
  hygon:
    enabled: false
    image: "harbor.4pd.io/rise-io/camp/dcu-device-plugin:1.0.7"
    imagePullPolicy: IfNotPresent
    nodeSelector:
      dcu: "on"
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
  metax:
    enabled: false
  enflame:
    enabled: false
    image: "quanzhenglong.com/camp/enflame/gcu-k8s-device-plugin:v2.0.30"
    imagePullPolicy: IfNotPresent
    nfdImage: "quanzhenglong.com/camp/enflame/node-feature-discovery:v1.2.20"
    gcufdImage: "quanzhenglong.com/camp/enflame/gcu-feature-discovery:v1.3.21"
    nodeSelector:
      gcu: "on"
    tolerations: {}
  enflameshare:
    enabled: false
    image: "quanzhenglong.com/camp/enflame/gcushare-device-plugin:2.2.5"
    imagePullPolicy: IfNotPresent
    nodeSelector:
      gcushare: "on"
    tolerations: {}
    sliceCount: 6
  alibaba:
    enabled: false

exporter:
  nvidia:
    enabled: false
    image:
      repository: nvcr.io/nvidia/k8s/dcgm-exporter
      pullPolicy: IfNotPresent
      # Image tag defaults to AppVersion, but you can use the tag key
      # for the image tag, e.g:
      tag: 3.3.6-3.4.2-ubuntu22.04
    dcgm:
      image:
        repository: harbor.4pd.io/rise-io/camp/dcgm
        pullPolicy: IfNotPresent
        tag: 3.3.6-1-ubuntu22.04
      # 是否主机上运行nv-hostengine进程，部署dcgm-exporter之前需要确认主机上是否有nv-hostengine进程，如果有，需要设置为true，
      # 如果没有，需要设置为false，设置为false时，dcgm-exporter会自动以sidecar的方式部署nv-hostengine进程
      isHostRunningNvHostEngineProcess: false
    arguments: []
    # Change the following reference to "/etc/dcgm-exporter/default-counters.csv"
    # to stop profiling metrics from DCGM
    # arguments: ["-f", "/etc/dcgm-exporter/dcp-metrics-included.csv"]
    # NOTE: in general, add any command line arguments to arguments above
    # and they will be passed through.
    # Use "-r", "<HOST>:<PORT>" to connect to an already running hostengine
    # Example arguments: ["-r", "host123:5555"]
    # Use "-n" to remove the hostname tag from the output.
    # Example arguments: ["-n"]
    # Use "-d" to specify the devices to monitor. -d must be followed by a string
    # in the following format: [f] or [g[:numeric_range][+]][i[:numeric_range]]
    # Where a numeric range is something like 0-4 or 0,2,4, etc.
    # Example arguments: ["-d", "g+i"] to monitor all GPUs and GPU instances or
    # ["-d", "g:0-3"] to monitor GPUs 0-3.
    # Use "-m" to specify the namespace and name of a configmap containing
    # the watched exporter fields.
    # Example arguments: ["-m", "default:exporter-metrics-config-map"]
    # Overrides the chart's name
    nameOverride: ""
    # Overrides the chart's computed fullname
    fullnameOverride: ""
    # Overrides the deployment namespace
    namespaceOverride: ""
    # Defines the runtime class that will be used by the pod
    runtimeClassName: ""
    # Defines serviceAccount names for components.
    serviceAccount:
      # Specifies whether a service account should be created
      create: true
      # Annotations to add to the service account
      annotations: {}
      # The name of the service account to use.
      # If not set and create is true, a name is generated using the fullname template
      name:
    rollingUpdate:
      enable: true
      # Specifies maximum number of DaemonSet pods that can be unavailable during the update
      maxUnavailable: 1
      # Specifies maximum number of nodes with an existing available DaemonSet pod that can have an updated DaemonSet pod during during an update
      maxSurge: 0
    # Labels to be added to dcgm-exporter pods
    podLabels: {}
    # Annotations to be added to dcgm-exporter pods
    podAnnotations: {}
    # Using this annotation which is required for prometheus scraping
    # prometheus.io/scrape: "true"
    # prometheus.io/port: "9400"
    # The SecurityContext for the dcgm-exporter pods
    podSecurityContext: {}
    # fsGroup: 2000
    # The SecurityContext for the dcgm-exporter containers
    securityContext:
      runAsNonRoot: false
      runAsUser: 0
      capabilities:
        add: ["SYS_ADMIN", "NET_ADMIN"]
      # readOnlyRootFilesystem: true
    # Defines the dcgm-exporter service
    service:
      # When enabled, the helm chart will create service
      enable: true
      type: ClusterIP
      port: 9400
      address: ":9400"
      # Annotations to add to the service
      annotations: {}
    # Allows to control pod resources
    resources: {}
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi
    serviceMonitor:
      enabled: true
      interval: 15s
      honorLabels: false
      additionalLabels: {}
      #monitoring: prometheus
      relabelings:
        - action: replace
          sourceLabels:
            - __meta_kubernetes_pod_node_name
          targetLabel: node
        - action: replace
          sourceLabels:
          - __meta_kubernetes_pod_node_name
          targetLabel: pod_node
        - action: labeldrop
          regex: (service|endpoint)
    #nodeSelector:
    #  gpu: "on"
    nodeSelector: {}
    tolerations: []
    #- operator: Exists
    affinity: {}
    #nodeAffinity:
    #  requiredDuringSchedulingIgnoredDuringExecution:
    #    nodeSelectorTerms:
    #    - matchExpressions:
    #      - key: nvidia-gpu
    #        operator: Exists
    extraHostVolumes: []
    #- name: host-binaries
    #  hostPath: /opt/bin
    extraConfigMapVolumes:
      - name: exporter-metrics-volume
        configMap:
          name: exporter-metrics-config-map
    extraVolumeMounts:
      - name: exporter-metrics-volume
        mountPath: /etc/dcgm-exporter/metrics.csv
        subPath: metrics.csv
        readOnly: true
    extraEnv:
      - name: DCGM_EXPORTER_COLLECTORS
        value: "/etc/dcgm-exporter/metrics.csv"
    # Path to the kubelet socket for /pod-resources
    kubeletPath: "/var/lib/kubelet/pod-resources"
    remoteEnable: false
    priorityClassName: ""
    hostNetwork: false
  kunlunxin:
    enabled: false
    image: "iregistry.baidu-int.com/kunlunxin/xpu_exporter:v5.0.2-alpha.6.vf" #这个镜像目前是昆仑芯内部开发的版本
    imagePullPolicy: IfNotPresent
    serviceMonitor:
      enabled: true
    nodeSelector:
      xpu: "on"
    tolerations: []
    service:
      labels: []
      annotations: []
      httpPort: 9508
    additionalLabels: []
  cambricon:
    enabled: false
    image: "harbor.4pd.io/rise-io/camp/cambricon-mlu-exporter:v2.0.11"
    imagePullPolicy: IfNotPresent
    nodeSelector:
      mlu: "on"
    tolerations: {}
    service:
      httpPort: 30108
  hygon:
    enabled: false
    image: "image.sourcefind.cn:5000/dcu/admin/base/dcu-exporter:v2.0.1"
    imagePullPolicy: IfNotPresent
    nodeSelector:
      dcu: "on"
    tolerations: {}
    service:
      httpPort: 16080
  iluvatar:
    enabled: false
    image: "quanzhenglong.com/iluvatar/ix-exporter:4.3.0-x86_64"
    imagePullPolicy: IfNotPresent
    nodeSelector:
      iluvatar: "on"
    tolerations: {}
    logLevel: 4
    containerPort: 32021
    servicePort: 32021
  enflame:
    enabled: false
    image: "quanzhenglong.com/camp/enflame/gcu-exporter:v1.5.12"
    imagePullPolicy: IfNotPresent
    serviceMonitor:
      enabled: true
    additionalLabels:
        release: prometheus
    nodeSelector:
      gcu: "on"
    tolerations: {}
    service:
      httpPort: 9400
  ascend: {}